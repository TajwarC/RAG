{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769eaf38",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe8d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6305c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_source', 'author': 'test_author', 'pages': 1, 'date_created': '2024-06-01'}, page_content='This is a test document.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(page_content=\"This is a test document.\",\n",
    "               metadata={\n",
    "                   \"source\": \"test_source\",\n",
    "                   \"author\": \"test_author\",\n",
    "                   \"pages\":1,\n",
    "                   \"date_created\":\"2024-06-01\"})\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da10f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple txt file \n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bcd0b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae900d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "# text loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7fdb129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98bba6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 0}, page_content='C H E M I C A L P H Y S I C S\\nCopyright © 2018\\nThe Authors, some\\nrights reserved;\\nexclusive licensee\\nAmerican Association\\nfor the Advancement\\nof Science. No claim to\\noriginalU.S.Government\\nWorks. Distributed\\nunder a Creative\\nCommons Attribution\\nLicense 4.0 (CC BY).\\nDeep neural network processing of DEER data\\nSteven G. Worswick1, James A. Spencer1, Gunnar Jeschke2, Ilya Kuprov1*\\nThe established model-free methods for the processing of two-electron dipolar spectroscopy data [DEER (double\\nelectron-electron resonance), PELDOR (pulsed electron double resonance), DQ-EPR (double-quantum electron para-\\nmagnetic resonance), RIDME (relaxation-induced dipolar modulation enhancement), etc.] use regularized fitting. In\\nthis communication, we describe an attempt to process DEER data using artificial neural networks trained on large\\ndatabases of simulated data. Accuracy and reliability of neural network outputs from real experimental data were\\nfound to be unexpectedly high. The networks are also able to reject exchange interactions and to return a measure\\nof uncertainty in the resulting distance distributions. This paper describes the design of the training databases, dis-\\ncusses the training process, and rationalizes the observed performance. Neural networks produced in this work are\\nincorporated as options into Spinach and DeerAnalysis packages.\\nINTRODUCTION\\nDouble electron-electron resonance (DEER), sometimes called pulsed\\nelectron double resonance (PELDOR), is a magnetic resonance exper-\\niment used to measure nanometer-scale distances between unpaired\\nelectrons in naturally paramagnetic or paramagnetically tagged systems\\n(1, 2). Extraction of distance information is possible because interelec-\\ntron dipolar interaction energy is proportional to the inverse cube of the\\ndistance. Unlike scattering and diffraction methods, DEER does not re-\\nquire long-range order in the sample; it can be applied to a variety of\\nsystems that may not crystallize (3, 4)—from molecular conductors (5)\\nall the way to proteins and nucleic acids (6, 7). Related methods, such as\\ndouble-quantum electron paramagnetic resonance (DQ-EPR) (8, 9)\\nor relaxation-induced dipolar modulation enhancement (RIDME)\\n(10, 11), provide similar information. From a theoretical standpoint,\\nDEER is quite straightforward: Its dipolar modulation signal factorizes\\ninto spin pair contributions, dipolar interactions with remote spins are\\nthe only significant signal decay mechanism, and the broadening caused\\nby that decay can be deconvolved because the decay function is available\\nfrom the unmodulated background (12).\\nDEER spectroscopy involves recording a dipolar modulation signal be-\\ntween two unpaired electrons and running regularized fitting to extract the\\ndistance distribution (13, 14). The procedure works well in spin-½ systems\\n(15), but significant complications arise when (i) more than two electron\\nspins are present (16, 17), (ii) the total spin of any paramagnetic center ex-\\nceeds ½ (18, 19), (iii) large interaction tensor anisotropies generate orien-\\ntation selection effects (20, 21), (iv) the system has microsecond-scale\\ninternaldynamics,and(v)thesystemhassignificantinterelectronexchange\\ncoupling (22, 23). Some of these matters are exceedingly hard to resolve or\\nwork around. It is also becoming clear that ab initio modeling and fitting of\\nevery possible complication are out of the question.\\nIn this communication, we report an attempt to train deep neural\\nnetworks to convert DEER signals into spin label distance distributions.\\nDEER data processing is well suited for the application of supervised\\nlearning techniques because it is a simple “vector-in, vector-out” regres-\\nsion problem (24). We used a large training database of synthetic DEER\\ntraces computed using Spinach (25) from randomly generated realistic\\ndistance distributions with a variable baseline and a variable amount of\\nnoise. The objective is to train networks that would recognize and work\\naround all of the issues mentioned above; here, we address complicated\\ndistancedistributions,exchange coupling,baselinedistortions,andnoise.\\nWe found that neural networks successfully process previously un-\\nseen experimental data in the presence of exchange coupling, as well as\\nrealistic amounts of noise and baseline signal. They are also able to pro-\\nvide a measure of confidence in the output. Once the training process is\\nfinished, the networks have no adjustable parameters. In cases where a\\nstable or a regularizable solution exists in principle, we expect that neural\\nnetworks should eventually be able to solve most of the above problems\\n(i) to (v) when they are trained on a database of sufficient size and scope.\\nDEER data processing—State of the art\\nFor an isolated electron pair, at a distance r with isotropic magnetogyric\\nratios g1 and g2, the echo modulation signal has the following form (see\\nthe Supplementary Materials for detailed derivations)\\nsðr; q; tÞ ¼ cos½ðD½1 \\x02 3cos2ðqÞ\\x03 þ JÞt\\x03;\\nD ¼ m0\\n4p\\ng1g2ℏ\\nr3\\nð1Þ\\nwhere J is the exchange coupling (nuclear magnetic resonance conven-\\ntion) and q is the angle between the interelectron direction and the\\nmagnet field. A typical experimental system is a frozen glass with all\\norientations equally likely. Integrating Eq. 1 over all angles produces\\na function known as the DEER kernel\\ngðr; tÞ ¼\\nﬃﬃﬃﬃﬃﬃﬃ\\np\\n6Dt\\nr\\n\"\\ncos½ðD þ JÞt\\x03FrC\\nﬃﬃﬃﬃﬃﬃﬃ\\n6Dt\\np\\nr\\n\"\\n#\\nþ\\nsin½ðD þ JÞt\\x03FrS\\nﬃﬃﬃﬃﬃﬃﬃ\\n6Dt\\np\\nr\\n#\\n\"\\n#\\nð2Þ\\nin which FrC and FrS are Fresnel’s cosine and sine functions. For an\\nensemble of isolated spin-½ pairs, the experimentally observed DEER\\ntrace is an integral of the kernel over the distance distribution\\ndðtÞ ¼ ∫\\n∞\\n0\\npðrÞgðr; tÞdr\\nð3Þ\\nEven in this ideal case, the relationship between the distance distri-\\nbution p(r) and the experimental signal d(t) is not straightforward: It is\\nan integral whose inversion is an ill-posed problem.\\n1School of Chemistry, University of Southampton, Highfield Campus, Southampton,\\nSO17 1BJ, UK. 2Department of Chemistry and Applied Biosciences, Swiss Federal Insti-\\ntute of Technology in Zurich, Vladimir Prelog Weg 2, CH-8093 Zürich, Switzerland.\\n*Corresponding author. Email: i.kuprov@soton.ac.uk\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n1 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 1}, page_content='The most popular procedure for extracting distance distributions\\nfrom DEER traces of real systems (13–15) rests on a number of significant\\nassumptions. The primary one is the dilute spin pair approximation—it is\\nassumed that the dipolar evolution function d(t) may be modeled as a\\nlinear combination of DEER traces of systems involving point electrons\\nat specific distances (4). Equation 2 is strictly valid only for spin-½ para-\\nmagnetic centers. For higher spin quantum numbers, this model only ap-\\nplies in the absence of level mixing and when overtone transitions during\\nthe pump pulse can be neglected. Exchange coupling is also commonly\\nignored, which often, but not always, provides a good approximation at\\ndistances longer than 15 Å (22).\\nThe next assumption deals with nonideal pulses and the inevitable\\npresence of external interactions. For dilute spin pairs, the experimental\\nDEER signal vexp(t) can be approximated as\\nvexpðtÞ ¼ ½1 \\x02 l þ ldðtÞ\\x03bðtÞ þ nðtÞ\\nð4Þ\\nwhere n(t) is the instrument noise, l is the spin-flip probability under\\nthe action of the pump pulse (12, 26), and b(t) is the intermolecular\\nbackground function—usually a stretched exponential\\nbðtÞ ¼ exp½\\x02ðktÞN=3\\x03\\nð5Þ\\nthat corresponds to a homogeneous distribution of distant spins in a\\nspace with dimension N (27). Equation 5 is also a good approximation\\nfor a homogeneous distribution in three dimensions with some ex-\\ncluded volume around the observer molecule (28). Along with relaxa-\\ntion, the background function limits the observation time and puts an\\nupper limit on the distances that can be measured (12, 29).\\nEven after b(t) and l are obtained by fitting, the mapping back from\\nd(t) into p(r) is still unstable—an infinitesimally small variation in d(t)\\ncan cause a finite variation in p(r). Tikhonov regularization is therefore\\ncommonlyused,inwhichtheambiguity isremovedbyrequiringthe sec-\\nond derivative of the solution to have the minimum norm (13, 14, 30).\\nThis requirement incorporates the physical wisdom that the solution\\nmust be smooth and sparse. The combined fitting error functional is\\nW½pðrÞ\\x03 ¼ ‖dexpðtÞ \\x02 ∫\\n∞\\n0\\npðrÞgðr; tÞdr‖\\n2\\nþ a‖\\nd2\\ndr2 pðrÞ‖\\n2\\nð6Þ\\nwhere a is the regularization parameter, chosen using the L-curve\\nmethod (14, 31). Other regularization methods have also been tried\\nand generally found to be successful (32, 33).\\nRegularization makes the problem tractable, but some distortions\\nare inevitable: Narrow features are broadened, and broad features\\nare artificially split. The error minimization runs within a reasonable\\nlength of time when an analytical expression for g(r, t) is available. When\\nthat is not the case (for example, in high-spin systems), the process\\nbecomes impractically slow, even on the latest computing hardware and\\nsoftware (19).\\nWhen the experimental DEER trace and the associated distance\\ndistribution are discretized on finite grids, Eq. 6 acquires a matrix-\\nvector form\\nW½p\\x03 ¼ ‖dexp \\x02 Gp‖\\n2 þ a‖D2p‖\\n2\\nð7Þ\\nwhere G is the matrix form of the DEER kernel integral and D is a\\nderivative matrix—for example, a finite difference one. At this point,\\nwe have a standard Tikhonov problem with a non-negativity con-\\nstraint that is also encountered elsewhere in magnetic resonance\\n(34, 35). Bayesian methods exist for uncertainty estimation (36), and\\nthe widely used DeerAnalysis package includes a validation tool (12).\\nThe regularized fitting method, as illustrated in Fig. 1, works very\\nwell for simple spin-½ systems (37, 38). Limited workarounds are avail-\\nable for situations when the core assumptions behind Eqs. 1 to 5 do not\\nhold. For multispin systems, data closer to the isolated spin pair approx-\\nimation can be obtained by intentionally reducing modulation depth\\n(16), by power scaling (17), or by sparse spin labeling (39). For Gd(III)\\nwith spin 7/2, researchers have demonstrated that distortions caused\\nby level mixing can be reduced by large frequency offsets between\\npump and observe pulses (40) or by RIDME (41). The latter technique\\nintroduces overtones of the dipolar frequency (42) that require a mod-\\nified DEER kernel with overtone coefficients that must be calibrated\\n(43). Deviations from the isotropic distribution of the spin-spin vector\\nby orientation selection can be partially averaged by varying the\\nmagnetic field at constant pump and observe frequencies (37, 44). In\\nsome site-directed spin labeling applications, an experimental estimate\\nof the background can be obtained by measuring singly labeled\\nconstructs (15). Significant progress was also recently made with Mellin\\ntransform techniques (45) that are likely to improve further once the\\nnon-negativity constraint is introduced.\\nConnection to neural networks\\nThe previous section describes a process that alternates matrix-vector\\noperations with nonlinear constraints—a good match to the algebraic\\nstructure of a feedforward neural network (46)\\nxn ¼ gnðWnxn\\x021 þ ynÞ\\nð8Þ\\nwhere the nthneuron layeracceptsan input vector xn −1, multipliesit by\\na weight matrix Wn, adds a bias vector yn, and passes the result through\\na nonlinear transfer function gn. This similarity is not strictly necessary—\\nMcCulloch and Pitts (47) showed that neural networks can compute any\\narithmetical or logical function. Multilayer feedforward networks are\\nknown to be universal approximators (46), but the present case is partic-\\nularly appealing because the required network is likely to be quite small.\\nDEER signals contain true dipolar oscillation, a background signal,\\nand a noise track that are statistically independent. The task of recon-\\nstructing a distance distribution can therefore be broken down into\\nperforming, in the least-squares sense, the following operations\\n½1 \\x02 l þ ldi \\x03⊙bj ¼ N\\x021ð½1 \\x02 l þ ldi\\x03⊙bj þ nkÞ\\n∀i; j; k\\ndi ¼ B\\x021ð½1 \\x02 l þ ldi\\x03⊙bjÞ\\n∀i; j\\npi ¼ G\\x021di\\n∀i\\nð9Þ\\nwhere ⊙denotes element-by-element multiplication, N−1 may be called\\n“denoising,” B−1 may be called “background rejection,” and G−1 may be\\ncalled “interpretation.” All three operations are not necessarily described\\nby matrices, are ill-posed, and only exist in the least-squares sense over an\\ninfinitely large number of instances of the true DEER signal vi, the\\nbackground signal bj, and the noise signal nk.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n2 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 2}, page_content='All three operationsare linear with respect to the dipolar modulation\\nsignal and are nonlinear with respect to the background and the noise.\\nThey map well into Eq. 8 and the neural network training process. Large\\ndatabases of {pi, di, bj, nk} can be generated using Spinach (25), and the\\nnetworks performing N−1, B−1, and G−1 can be obtained using backpro-\\npagation training (48, 49).Thesenetworksare calledmapping networks;\\nthey are extensively researched (46, 47, 50).\\nAt a more general level, neural network “surrogate” solutions to\\nFredholm equations are well researched in their own right (51), with\\nrigorous accuracy bounds available (52, 53). In 2013, Jafarian and Nia\\n(54) proposed a two-layer feedback network built around a Taylor ex-\\npansion of the solution; Effati and Buzhabadi (55) published a feed-\\nforward network proposition. Both groupsconsidereda generic Fredholm\\nequation without any specific physical model or context. At that time,\\nneither group had the computing power to train a network of sufficient\\nwidth and depth to perform the tasks encountered in this work. How-\\never, both groups observed that, for such problems as they could handle,\\nneural networks provided very accurate solutions (54, 55). Promising\\nneural network results also exist for two-dimensional (2D) integral\\nequations (56, 57), meaning that processing triple electron resonance\\nspectroscopy (58) data with neural networks may also be possible.\\nMATERIALS AND METHODS\\nTraining database generation\\nNeural network training requires a library of inputs and their corre-\\nsponding outputs covering a range that is representative of all possi-\\nbilities (48, 49, 59). Real distance distributions between spin labels are\\nrarely known exactly and, therefore, collating experimental data is not\\nan option. Fortunately, high-accuracy simulations, taking into account\\nmost of the relevant effects, have recently become possible (19, 25, 60).\\nThey can be time-consuming (19) but only need to be run once to gen-\\nerate multiple simulated DEER traces with different artificial noise and\\nbackground functions. These traces are then stored in a database\\nalongside the “true” distance distributions they were generated from.\\nAn example is shown in Fig. 2.\\nThe size and shape of the training database are entirely at the trainer’s\\ndiscretion—a wide variety of spin systems, parameter ranges, secondary\\ninteractions, and instrumental artifacts may be included. This explor-\\natory work uses the DEER kernel for a pair of spin-½ particles, but\\nthe DEER simulation module in Spinach is not restricted in any way\\n(60)—training data sets may be generated for any realistic combinations\\nof spins, interactions, and pulse frequencies. The following parameters\\nare relevant:\\nFig. 1. Standard Tikhonov regularization processing, illustrated using site pair V96C/I143C in the lumenal loop of a double mutant of LHCII, with iodoacetamido-\\nPROXYL spin labels attached to the indicated cysteines (64). For the primary data (top left), the zero time (green vertical line) is determined using moment analysis in the\\nvicinity of the intensity maximum. The optimal starting time for background fitting (blue vertical line) is determined by minimizing probability density at the maximum\\ndistance. Data have been cut by 400 ns at the end (red vertical line) to minimize the influence of the artifact arising from overlapping pump and observe pulse excitation\\nbands. The stretched exponential background fit is shown as a solid red line (where fitted) and as a dotted red line (where extrapolated). The background-corrected data\\n(form factor, black) are shown in the top right panel together with fits using the regularization parameter corresponding to the origin distance criterion (red) and maximum\\ncurvature criterion (green). These two choices are also indicated in the L-curve (bottom left). The bottom right panel shows distance distributions computed with these two\\nregularization parameters in matching color. Pastel background shading indicates distance ranges where the shape of the distribution is expected to be reliable (green),\\nwhere mean distances and widths are expected to be reliable (yellow), where only mean distances are expected to be reliable (orange), and where data should not be\\ninterpreted (red). These ranges are derived from the duration of the primary data (7).\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n3 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 3}, page_content='(1) Minimum and maximum distances in the distribution. Because\\nthe dipolar modulation frequency is a cubic function of distance, there is\\nascaling relationship between the distance range and the signalduration\\ntA\\nr3\\nA\\n¼ tB\\nr3\\nB\\nð10Þ\\nThe salient parameter here is the “dynamic range”—the ratio of the lon-\\ngest distance and the shortest. Training signals must be long enough\\nand discretized well enough to reproduce all the frequencies present.\\n(2) Functions used to represent distance peaks and their number. A\\nrandom number of skew normal distribution functions (61) with ran-\\ndom positions within the distance interval and random full widths at\\nhalf magnitude were used in this work\\npðxÞ ¼\\n2\\ns\\nﬃﬃﬃﬃﬃ\\n2p\\np\\ne\\x02ðx\\x02x0Þ2\\n2s2\\n∫\\na x\\x02x0\\ns\\nð\\nÞ\\n\\x02∞\\ne\\x02t2\\n2 dt\\nð11Þ\\nwhere s is the SD of the underlying normal distribution, x0 is the loca-\\ntion of its peak, and a is the shape parameter regulating the extent of the\\nskew. Distance distributions were integrated with the DEER kernel in\\nEq. 2 to obtain DEER form factors. We found that generating distance\\ndistributions with up to three peaks was sufficient to ensure that the\\nnetworks could generalize to an arbitrary number of distances (see\\nthe “Measures of uncertainty” section).\\n(3) Noise parameters and modulation depth. Because DEER traces\\nwere recorded in the indirect dimension of a pseudo-2D experiment, the\\nnoisewasnotexpectedtobecolored—thiswasconfirmedbyexperiments\\n(36). We used Gaussian white noise with the SD chosen randomly be-\\ntween zero and a user-specified fraction of the modulation depth, which\\nwas also chosen randomly from within the user-specified ranges.\\n(4) Background function model and its parameters. We used Eq. 5\\nwith the dimensionality parameter selected randomly from the user-\\nspecified range.\\n(5) Discretization grids in the time and the distance domains. The\\npoint count must be above the Nyquist condition for all frequencies\\nexpected within the chosen ranges of other parameters. The number of\\ndiscretization pointsdictatesthe dimension of the transfermatricesand bias\\nvectors in Eq. 8, which, in turn, determine the minimum training set size.\\n(6) Training set size. A fully connected neural network with n layers\\nof width k has n(k2 + k) parameters. Each of the “experimental” DEER\\ntraces is k points long, meaning that n(k + 1) is the absolute minimum\\nnumber of DEER traces in the training set. At least 100 times that\\namount is in practice necessary to generate high-quality networks.\\nThe parameter ranges entering the training data set are crucial for\\nthe success of the resulting network ensemble—the training data set\\nmust be representative of the range of distances, peak widths, noise am-\\nplitudes, and other attributes of the data sets being processed. The\\nparameters entering the current DEERNet training database generation\\nprocess are listed in Table 1.\\nReliable neural network training requires signals in the database to be\\nconsistently scaled and to fall within the dynamic range of the transfer\\n20\\n30\\n40\\n50\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nDistance distr.\\n0\\n0.5\\n1\\n1.5\\n2\\n–0.2\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nOutput\\nBG\\nNoise\\nDEER\\nAmplitude (a.u.)\\nDistance (Å)\\nTime (µs)\\nFig. 2. One of the millions of synthetic DEER data sets, generated using Spinach\\n(25)and used for neural network training inthiswork. (Left) Randomly generated\\ndistance distribution. (Right) The corresponding DEER form factor (purple), a ran-\\ndomly generated noise track (yellow), a randomly generated intermolecular\\nbackground signal (red, marked BG), and the resulting “experimental” DEER signal\\n(blue). a.u., arbitrary units.\\nTable 1. Training database generation parameters used in this work.\\nWhere a maximum value and a minimum value are given, the parameter\\nis selected randomly within the interval indicated for each new entry in\\nthe database. Ranges in the suggested values indicate recommended\\nintervals for the corresponding parameter.\\nParameter\\nSuggested values\\nMinimum distance in the distribution (Å)\\n10–15\\nMaximum distance in the distribution (Å)\\n50–80\\nDEER trace length (ms)\\n2–5\\nMinimum number of distance peaks\\n1–2\\nMaximum number of distance peaks\\n2–3\\nData vector size\\n256–1024\\nRMS noise, fraction of the modulation depth\\n0.05–0.10\\nMinimum exchange coupling (MHz)\\n−5.0\\nMaximum exchange coupling (MHz)\\n+5.0\\nMinimum background dimensionality\\n2\\nMaximum background dimensionality\\n3.5\\nMinimum full width at half magnitude for\\ndistance peaks, fraction of the distance\\n0.05–0.10\\nMaximum full width at half magnitude for\\ndistance peaks, fraction of the distance\\n0.20–0.50\\nMaximum shape parameter (Eq. 11)\\n+3.0\\nMinimum shape parameter (Eq. 11)\\n−3.0\\nMinimum modulation depth\\n0.05–0.10\\nMaximum modulation depth\\n0.50–0.60\\nMinimum background decay rate (MHz)\\n0.0\\nMaximum background decay rate (MHz)\\n0.5\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n4 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 4}, page_content='functions. The peak amplitude of each distance distribution was there-\\nfore brought by uniform scaling to 0.75, and all DEER traces were\\nuniformly scaled and shifted so as to have the first point equal to\\n1 and the last point equal to 0.\\nThe training process requires vast computing resources, but using\\nthe trained networks does not. For the networks and databases de-\\nscribed in this communication, the training process for a 100-network\\nensemble takes about a week on a pair of NVidia Tesla K40 cards. Once\\nthe training process is finished, the networks can be used without dif-\\nficulty on any computer strong enough to run MATLAB.\\nNetwork topology and the training process\\nThree simple types of feedforward network topologies explored in this\\nwork are shown in Fig. 3. Basic fixed width feedforward networks (top\\ndiagram) do, in practice, suffice, but we have also explored variable\\nwidth networks (middle diagram) and networks based on the stage\\nseparation discussed around Eq. 9. Specifically, it makes physical sense\\nto separate the form factor extraction stage from the DEER signal inter-\\npretation stage (Fig. 3, bottom diagram).\\nThe most common transfer functions in Eq. 8 are sigmoidal, map-\\nping [−∞, ∞] into [−1, 1]. However, distance distribution is a non-\\nnegative function, and we observed that including this fact at the\\nnetwork level improves performance. Using the strictly positive logistic\\nsigmoid function\\ngðxÞ ¼\\n1\\n1 þ e\\x02x\\nð12Þ\\nFig. 3. Schematic diagrams (produced by MATLAB) of the three types of neural network topologies explored in this work, using four-layer networks as an\\nexample. W block indicates multiplication by the weight matrix and b block indicates the addition of a bias vector. (Top) Fully connected full-width network. (Middle) Fully\\nconnected network with choke points. (Bottom) Functionally separated network with some layers explicitly dedicated to background rejection and others to interpretation—\\nduring the training process, the first output is the DEER form factor, and the second output is the distance probability density function.\\nTable 2. Performance statistics for a family of feedforward networks set up as a simple sequence of fully connected layers of the same width as the\\ninput vector. A schematic of the network topology is given in the top diagram of Fig. 3.\\nTask\\nNetwork\\nMean relative error\\nRelative error SD\\nIteration time*, Tesla K40 (s)\\nDistance distribution recovery\\nIn-(256)2-Out\\n0.090\\n0.231\\n0.32\\nIn-(256)3-Out\\n0.077\\n0.208\\n0.44\\nIn-(256)4-Out\\n0.070\\n0.195\\n0.74\\nIn-(256)5-Out\\n0.069\\n0.194\\n0.99\\nIn-(256)6-Out\\n0.069\\n0.192\\n1.19\\nForm factor recovery\\nIn-(256)2-Out\\n0.0065\\n0.0143\\n0.31\\nIn-(256)3-Out\\n0.0042\\n0.0094\\n0.51\\nIn-(256)4-Out\\n0.0037\\n0.0084\\n0.75\\nIn-(256)5-Out\\n0.0034\\n0.0080\\n0.98\\nIn-(256)6-Out\\n0.0034\\n0.0080\\n1.18\\n*Using a database with 100,000 DEER traces generated as described under “Training database generation” section.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n5 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 5}, page_content='at the last layer instead of the hyperbolic tangent function used by the\\ninner layers\\ngðxÞ ¼ ex \\x02 e\\x02x\\nex þ e\\x02x\\nð13Þ\\ndecreases both the final error and the training time (table S1).\\nThe training of all neural networks was carried out on NVidia Tesla\\nK20 and K40 coprocessor cards using MATLAB R2018a Neural\\nNetwork Toolbox and Distributed Computing Toolbox. Resilient back-\\npropagation (49) and scaled conjugate gradient (48) error minimization\\nmethodswereusedwiththeleast-squareserrormetric.Trainingdatabases\\nwere partitioned into a 70% training set (with respect to which the min-\\nimization was carried out), a 15% validation set (that was monitored to\\nprevent overfitting), and a 15% testing set with respect to which the\\nperformance figures were compiled; this is in line with standard practice.\\nUniform feedforward networks\\nThe simplest strategy for training a generic “vector-in, vector-out” neu-\\nral network is to set up a number of fully connected layers of the same\\nsize as the input vector, resulting in the topology shown in the top di-\\nagram of Fig. 3. The performance metrics for a family of such networks\\nare given in Table 2 and illustrated graphically in Figs. 4 and 5. The “re-\\nlative error” metric is defined as the 2-norm of the difference between\\n0\\n0.5\\n1\\n1.5\\n2\\nAn easy case\\nDEER\\nBG\\n0\\n0.5\\n1\\n1.5\\n2\\nA tough case\\nDEER\\nBG\\n0\\n0.5\\n1\\n1.5\\n2\\nA bad case\\nDEER\\nBG\\n20\\n30\\n40\\n50\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\nTrue\\nDEERNet\\n20\\n30\\n40\\n50\\nTrue\\nDEERNet\\n20\\n30\\n40\\n50\\nTrue\\nDEERNet\\nTime (µs)\\nTime (µs)\\nTime (µs)\\nDistance (Å)\\nDistance (Å)\\nDistance (Å)\\nAmplitude (a.u.)\\nAmplitude (a.u.)\\nFig. 4. Distance distribution recovery performance illustration for a five-layer feedforward neural network, fully connected, with 256 neurons per layer. All\\ninner layers have hyperbolic tangent transfer functions; the last layer has the strictly positive logistic sigmoid transfer function.\\n0\\n0.5\\n1\\n1.5\\n2\\nAn easy case\\nDEER\\nBG\\n0\\n0.5\\n1\\n1.5\\n2\\nA tough case\\nDEER\\nBG\\n0\\n0.5\\n1\\n1.5\\n2\\nA bad case\\nDEER\\nBG\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\nTime (µs)\\nTime (µs)\\nTime (µs)\\n0\\n0.5\\n1\\n1.5\\n2\\nDEER\\nBG\\n0\\n0.5\\n1\\n1.5\\n2\\nDEER\\nBG\\n0\\n0.5\\n1\\n1.5\\n2\\nDEER\\nBG\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\n0\\n0.5\\n1\\nTime (µs)\\nTime (µs)\\nTime (µs)\\nAmplitude (a.u.)\\nAmplitude (a.u.)\\nFig. 5. DEER form factor recovery performance illustration for a six-layer feedforward neural network, fully connected, with 256 neurons per layer. All layers\\nhave hyperbolic tangent transfer functions.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n6 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 6}, page_content='the network output and the true answer divided by the 2-norm of the\\ntrue answer.\\nIt is clear from the performance statistics that, for a single neural\\nnetwork, the average norm of the deviation drops below 10% of the total\\nsignal norm and stops improving once the network is five to six layers deep.\\nTraining iteration time depends linearly on the depth of the network.\\nThe data for the visual performance illustrations (Figs. 4 and 5) were\\nselected from the training database in the following way: the “easy case”\\nwas sampled from the relative error histogram region located between\\n0 and 1 SD; the “tough” case was sampled from the region between\\n1 and 2 SDs; the “bad case” was sampled from 100 worst fits in the\\nentire 100,000-trace training database. Performance illustrations for\\nthe rest of the networks reported in Table 2 are given in figs. S1 to\\nS3. Given that the bad cases are the worst 0.1% of the training data\\nset, the performance is rather impressive. Similar sequential improve-\\nments are observed for the networks tasked with the recovery of the\\nDEER form factor (Fig. 5).\\nFor the vast majority of DEER traces in the training database, the\\nrecovery of the form factor is close to perfect. Performance illustrations\\nfor the rest of the form factor recovery networks reported in Table 2 are\\ngiven in figs. S4 to S6.\\nFeedforward networks with choke points\\nExcellent as the performance of the neural networks in Table 2 and\\nFig. 4 may appear, deeper inspection still indicates that having 256\\nneurons in the inner layers may not be necessary, and this dimension\\ncan potentially be reduced. This is most obvious from the analysis of\\nsingular value decompositions (SVDs) of the weight matrices in Eq. 8.\\nThe general form of the SVD of a matrix W is\\nW ¼ ∑\\nk\\nskjuk〉〈vkj\\nð14Þ\\nwhere the right singular vectors 〈vk| may be viewed as a library of distinct\\ninput signals, the left singular vectors |uk〉may be viewed as the library of\\ndistinct output signals, and the singular values sk may be viewed as the\\namplification coefficients applied when an input is mapped into an\\noutput. If some singularvaluesare zero, then thecorresponding pathways\\nare unimportant and may be dropped. Mathematically, this means that\\nthe rank of the matrix is smaller than its dimension.\\nSingular values of all transfer matrices in a six-layer distance\\ndistribution recovery network are plotted in Fig. 6. It is clear that none\\nof the weight matrices are full rank, and the matrices occurring later in\\nthe network have fewer large-amplitude singular values. This suggests\\nthat intermediate layers could require fewer than 256 neurons. Because\\nthe corresponding singular values are small or zero, reducing the num-\\nber of neurons in intermediate layers is not expected to affect accuracy.\\nHowever, the reduction in the training time could be considerable: A\\nfully connected N-neuron layer has N2 + N adjustable parameters, and\\nsothebenefitofgoingdownfrom256neuronsto64orfewerissignificant.\\nThis is explored in detail in Table 3. Although the intuition provided\\nby Eq. 14 and Fig. 6 suggests that reducing the number of neurons in the\\nintermediate layers might be a good idea, this is not corroborated by the\\npractical performance figures. Any reduction in the dimension of\\nintermediate layers results in performance degradation. The position\\nof the choke point (table S2) does not appear to have any influence\\non the performance.\\nAnother architectural observation is that bias vectors do not appear\\nto be necessary in Eq. 8—networks trained without bias vectors have\\nidentical performance (table S2). An examination of the optimal bias\\nvectors does not yield any interpretable patterns. This is likely because\\n50\\n100\\n150\\n200\\n250\\nSingular value index\\n1\\n2\\n3\\n4\\n5\\n6\\nSingular value\\nLayer 1\\nLayer 2\\nLayer 3\\nLayer 4\\nLayer 5\\nLayer 6\\nFig. 6. Singular values of the weight matrices in a six-layer feedforward neural\\nnetwork, fully connected, with 256 neurons per layer, and trained as described in\\nthe main text. All inner layers have hyperbolic tangent transfer functions; the last\\nlayer has the strictly positive logistic sigmoid transfer function.\\nTable 3. Performance statistics for a family of feedforward networks set up as a simple sequence of fully connected layers with a choke point in the\\nmiddle. A schematic of the network topology is given in the middle diagram of Fig. 3.\\nNetwork\\nMean relative error\\nRelative error SD\\nIteration time*, Tesla K40 (s)\\nIn-256-32-256-Out\\n0.095\\n0.230\\n0.25\\nIn-256-64-256-Out\\n0.086\\n0.217\\n0.29\\nIn-256-128-256-Out\\n0.084\\n0.217\\n0.39\\nIn-256-256-256-Out\\n0.077\\n0.208\\n0.51\\nIn-(256)2-32-(256)2-Out\\n0.090\\n0.210\\n0.61\\nIn-(256)2-64-(256)2-Out\\n0.074\\n0.201\\n0.65\\nIn-(256)2-128-(256)2-Out\\n0.073\\n0.200\\n0.83\\nIn-(256)2-256-(256)2-Out\\n0.069\\n0.194\\n0.99\\n*Using a database with 100,000 DEER traces generated as described under “Training database generation” section.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n7 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 7}, page_content='the input and the output data are already well scaled (see “Training\\ndatabase generation” section) and fit into the dynamic window of the\\ntransfer functions without the need for any shifts. Still, the variational\\nfreedom afforded by the bias vectors appears to accelerate the training\\nprocess, and we have kept them for that reason.\\nStructured networks\\nTable 2 indicates that plain feedforward networks with more than six\\nlayers do not produce any further improvements in the performance. If\\nthose improvements are even possible, then more sophisticated topol-\\nogies must be used. One possibility is shown in the bottom diagram of\\nFig. 3—the first group of layers was trained against the form factor and\\ntherefore eliminated noise and background. That form factor was then\\nfed into the second group of layers, making the probability density ex-\\ntraction easier for those layers. In principle, structured networks may be\\nassembled from pretrained pieces. In the case of the bottom diagram of\\nFig. 3, the pieces would come from one of the form factor extraction\\nnetworks in Table 1 and a separate network trained to interpret\\nbackground-free form factors. Performance figures for networks of this\\ntype are given in Table 4.\\nUnfortunately, it does not appear that tailoring carries any advan-\\ntages relative to the data reported for the simple feedforward networks\\nin Table 2. Training a 12-layer network against two sets of outputs is\\nalso exceedingly expensive. We therefore used uniform feedforward\\nnetworks (Fig. 3, top) for all production calculations discussed below.\\nThe networks were trained on a data set where raw experimental data\\nwithout any preprocessing go in, and the distance distribution is\\nexpected at the output.\\nStill, the networks evaluated in Table 4 could potentially be beneficial\\nas a safety catch: Humans can easily recognize incorrect form factors\\nvisually and thus detect cases of neural networks failing, for example,\\nif they encounter a situation not covered by the training set.\\nMeasures of uncertainty\\nWhen applied correctly, the standard Tikhonov regularized DEER data\\nanalysis (12–14) produces clear results and easily interpretable distance\\ndistributions. However, when applied naively to corrupted or featureless\\ndata sets, it can result in overinterpretation of the data (12, 36, 38). In\\nparticular, less experienced practitioners may have difficulty dis-\\ntinguishing genuine distance peaks from artifacts (62). Feedback from\\nthe EPR community has led to the concept of a validation tool that\\nwould be able to identify corrupted or featureless DEER traces. These\\ntools exist within the Tikhonov framework (12, 36), although they can\\nbe computationally demanding. A similar tool is therefore required for\\nneural networks.\\nA {“good”, “bad”} classification network would be the obvious solu-\\ntion, but the amount of experimental DEER data in the world is rather\\nsmall—polling the community for examples of bad DEER traces is un-\\nlikely to return a data set of sufficient size. We therefore decided to pur-\\nsue another common alternative: to train an ensemble of neural\\nnetworks using different synthetic databases and to use the variation\\nin their outputs as a measure of uncertainty in the distance distribution\\n(63). Such a measure is useful in any case, and a large variation would\\nindicate uninterpretable input data.\\nTo investigate the performance of this approach in estimating dis-\\ntance distribution uncertainties and detecting corrupted data, we\\ntrained 100 five-layer networks on different databases (generated as de-\\nscribed under “Training database generation” section) and evaluated\\ntheir performance against a previously unseen database.\\nThe results are shown in Fig. 7. The relative error metric is the ratio\\nof the 2-norm of the difference between the output and the true\\nanswer divided by the 2-norm of the true answer. The “worst relative\\nerror” refers to the worst-case performance in the entire database.\\nPerformance metrics for all networks in the ensemble are plotted as\\nTable 4. Performance statistics for a family of tailored networks composed of a group of form factor extraction layers that form the input of the\\ninterpretation layers. A schematic of the network topology is given in the bottom diagram of Fig. 3. FF, form factor; Int, interpretation.\\nNetwork topology\\nInterpretation\\nForm factor extraction\\nMean relative error\\nRelative error SD\\nMean relative error\\nRelative error SD\\nIn-FF[(256)1]-Int[(256)1]-Out\\n0.276\\n0.467\\n0.355\\n0.383\\nIn-FF[(256)2]-Int[(256)2]-Out\\n0.103\\n0.236\\n0.040\\n0.083\\nIn-FF[(256)3]-Int[(256)3]-Out\\n0.094\\n0.225\\n0.021\\n0.046\\nIn-FF[(256)4]-Int[(256)4]-Out\\n0.087\\n0.216\\n0.015\\n0.028\\nIn-FF[(256)5]-Int[(256)5]-Out\\n0.081\\n0.196\\n0.012\\n0.023\\nIn-FF[(256)6]-Int[(256)6]-Out\\n0.080\\n0.192\\n0.012\\n0.022\\n0.018\\n0.02\\n0.022\\n0.024\\n0.026\\n0.028\\n0.03\\nMean relative error\\n1\\n1.5\\n2\\n2.5\\n3\\n3.5\\n4\\n4.5\\n5\\nWorst relative error\\nAll nets\\nGood nets\\nGood net average\\nFig. 7. Performance of an ensemble of 100 five-layer neural networks on a pre-\\nviously unseen database. Each of the networks was started from a different random\\ninitial guess and trained in a different randomly generated database. Red dots indicate\\nthe good networks that are better than the median on both the mean relative error\\nand the worst relative error. The blue asterisk is the performance of the average output\\nof the good networks.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n8 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 8}, page_content='red circles. The networks that scored better than the median on both\\ncharacteristics are labeled good and additionally marked with a dot.\\nThe performance of the arithmetical mean of the outputs of good\\nnetworks is shown as a blue asterisk. The SD of the mean across the\\ngood network ensemble is a measure of uncertainty in the output (Fig. 8).\\nIn practice, the mean output signal and the SD are computed for each\\npoint and plotted in the form of 95% confidence bounds, as shown in the\\nfigures presented in the next section. A more detailed investigation of the\\neffect of the noise in the input data on the reconstruction quality and the\\nconfidence intervals is given in section S5.\\nAn important practical test of correctness, intended to distinguish a\\nneural network that merely fits a few Gaussians to the data set from a\\nnetwork that is a Fredholm solver, would be to present a DEER trace\\nwith four distances to a network that was trained on a database with at\\nmost three. A network that has learned to be a Fredholm solver in the\\nsense discussed in (51, 52, 54, 55, 57) should still return the right answer.\\nAs Fig. 9 illustrates, our networks pass that test.\\nRESULTS AND DISCUSSION\\nThis section contains a demonstration of the practical performance of\\nneural network ensembles for distance distribution reconstruction and\\nuncertainty analysis. The results from the best current Tikhonov\\nmethod implementation (15) are provided as a reference.\\nTest case library\\nDEER is used most widely in structural biology on doubly spin-labeled\\nproteins, nucleic acids, and their complexes. In some cases, distance dis-\\ntributions are narrow and give rise to time-domain data with several\\nobservable oscillations. As an example, we use DEER data for site\\npair 96/143 in the monomeric plant light-harvesting complex II\\n(LHCII; sample I) (64). When intrinsically disordered domains are\\npresent, distance distributions can be very broad. This applies to site\\npair 3/34 in LHCII (sample II) (64). Even narrower and broader distri-\\nbutions are found in polymer science. We encountered the smallest\\nwidth-to-distance ratio in a short oligo-phenyleneethynylene end-labeled\\nwith a rigid nitroxide label (sample III) (37). One of the broadest dis-\\ntributions for which we have high-quality DEER data was observed\\nin a [2]catenane spin-labeled on both of the intertwined macrocycles\\n(sample IV) (65). As an example, where a narrow and a broad dis-\\ntance distribution peak are simultaneously present, we use decorated\\ngold nanoparticles (sample V) (66). As a typical example for the distri-\\nbutions encountered in large rigid organic molecules, we use a doubly\\nlabeled phenyleneethynylene molecule (sample VI) (16).\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n20\\n30\\n40\\n50\\nDistance (Å)\\nProbability density (a.u.)\\n20\\n30\\n40\\n50\\nDistance (Å)\\n20\\n30\\n40\\n50\\nDistance (Å)\\nAn easy case\\nA tough case\\nA bad case\\nFig. 8. Network ensemble performance illustration. Easy (left), tough (middle), and worst-case (right) agreement on the training set data. The variation in the outputs of\\ndifferent neural networks within the ensemble is a measure of the uncertainty in their output (63) when the training databases are comprehensive.\\n0\\n0.5\\n1\\n1.5\\n2\\n0.8\\n0.6\\n1.0\\nInput data\\n20\\n30\\n40\\n50\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nRight answer\\nEnsemble statistics\\n20\\n30\\n40\\n50\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\n0.4\\n0.2\\n0.0\\nFig. 9. A demonstration that deep neural networks learn to be Fredholm solvers rather than model fitters. Presenting a data set with four distances to networks\\ntrained on the database with at most three distances yields the right answer with high confidence. All networks in the ensemble return four peaks.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n9 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 9}, page_content='Fig. 10. Distance distributions obtained by Tikhonov regularization (blue lines) and uncertainties estimated by the DeerAnalysis validation tool (pink areas)\\nfor the six experimental test cases. (A) Site pair V96C/I143C in the lumenal loop of a double mutant of LHCII, with iodoacetamido-PROXYL spin labels attached to the\\nindicated cysteines (64); (B) site pair S3C/S34C in the N-terminal domain of a double mutant of the LHCII monomers, with iodoacetamido-PROXYL spin labels attached\\nto the indicated cysteines (64); (C) end-labeled oligo(para-phenyleneethynylene)—a rigid linear molecule described as compound 3a in (37); (D) [2]catenane (a pair of\\nlarge interlocked rings) with a nitroxide spin label on each ring described as sample II in (65); (E) pairs of nitroxide radicals tethered to the surface of gold nanoparticles,\\nwith the thiol tether attachment points diffusing on the surface of the nanoparticle, sample Au3 after solvolysis and heating in (66); (F) rigid molecular triangle labeled\\nwith nitroxide radicals on two corners out of three, sample B11inv in (16).\\n0\\n2\\n4\\n0\\n0.5\\n1\\n30\\n40\\n50\\n60\\n30\\n40\\n50\\n60\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nNetwork ensemble\\nEnsemble statistics\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\nExperimental data\\nFig. 11. DEERNet performance on sample I: A site pair V96C/I143C in the lumenal loop of a double mutant of LHCII, with iodocateamido-PROXYL spin labels attached\\nto the indicated cysteines (64). Residue 96 is located in the lumenal loop, and residue 143 is a structurally rigid “anchor” position in the protein core. In agreement with\\nthe results reported in the original paper, a bimodal distance distribution is measured—indicating flexibility in the lumenal loop. The low-confidence peak around 57 Å\\nlikely results from protein aggregation.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n10 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 10}, page_content='Experimental data preprocessing\\nWe preprocessed all primary data in DeerAnalysis (12). We accepted\\nthe zero time of the dipolar oscillation and signal phase determined auto-\\nmatically by DeerAnalysis. We cut off the last 400 ns of each trace to re-\\nmove the “2 + 1” end artifact that arises from excitation band overlap\\nof pump and observe pulses (7). For sample III, a part of the end artifact\\nwas still visible, and the last 800 ns had to be cut off. These data were\\nsupplied to DEERNet, which expects a column vector containing the\\ntime axis (from 0 to tmax) in microseconds and a column vector of the\\ncorresponding DEER signal amplitudes. Internally, the signal is shifted\\nand scaled to match the dynamic range of the network, and down-\\nsampled with a matched quadratic Savitzky-Golay filter to make the\\nnumber of points equal to the number of neurons in the input layer.\\nThe trace length tmax is used in Eq. 10 to determine the distance axis.\\nFor comparison, we also fully processed the data using DeerAnalysis\\n(Fig. 10). We applied default background fitting, assuming a homoge-\\nneous spatial distribution (n = 3), except for sample III, where n was\\nfitted. This exception was required because we averaged the data for\\nsample III over 37 different observer fields to reduce orientation selec-\\ntion effects; this averaging causes nonexponential background decay.\\nWe found n = 3.40 for that sample. We then computed the L-curve\\nin all cases. The default choice of the optimum regularization parameter\\n(minimum distance to the origin) was accepted unless it differed clearly\\nfrom the maximum curvature point and the back-predicted DEER data\\nwere clearly overdamped compared to the experimental curve. In this\\ncase, whichwasencounteredfor Sample I (see Fig. 1) and III, we selected\\nthe maximum curvature point.\\nWe performed Monte Carlo validation by varying the noise (twice\\nthe original noise level, 11 instances) and the starting time of the\\nbackground fit (from 240 ns to half the maximum time, 11 instances),\\ngiving a total of 121 Monte Carlo instances. For Sample III, we also\\nvaried the background dimension from 2.6 to 3.6 (11 instances) and re-\\nduced the number of noise instances to two per background starting\\ntime/dimension pair, giving a total of 242 instances. We pruned valida-\\ntion data at the default levelof 1.15, meaning that all solutions witha root\\nmean square deviation (RMSD) of the fit from the background-corrected\\n0\\n2\\n4\\n6\\n0\\n0.5\\n1\\n30\\n40\\n50\\n60\\n70\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nNetwork ensemble\\nEnsemble statistics\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\nExperimental data\\n30\\n40\\n50\\n60\\n70\\nFig. 12. DEERNet performance on sample II: A site pair S3C/S34C in the N-terminal domain of a double mutant of the LHCII, with iodoacetamido-PROXYL spin\\nlabels attached to the indicated cysteines (64). The data stem from LHCIII monomers. Residue 3 is located in the very flexible N-terminal region, while residue 34 is\\nlocated in the structured part of the N-terminal domain.\\n0\\n5\\n10\\n0\\n0.5\\n1\\n40\\n60\\n80\\n40\\n60\\n80\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nNetwork ensemble\\nEnsemble statistics\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\nExperimental data\\nFig. 13. DEERNet performance on sample III: End-labeled oligo(para-phenyleneethynylene)—a rigid linear molecule described as compound 3a in (37). The\\nmaximum and the width of the distance distribution are in close agreement with the Tikhonov regularization results, whereas the expected skew of the distribution is\\nnot reproduced. Notably, there are no low-intensity artifacts that the Tikhonov method produces around the baseline.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n11 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 11}, page_content='data exceeding 1.15 times the minimum RMSD were excluded. In all\\ncases, this pruning led to only a slight reduction of the uncertainty\\nestimate. For Sample V, we also fitted the model of biradicals distributed\\non the surface of spherical particles with a Gaussian distribution of the\\nparticle radius (model Chechik2 in DeerAnalysis) (16). We found a bi-\\nradical distance of 1.87 nm with an SD of 0.22 nm and a fraction of\\n0.72 for the biradical distance contribution. The particle mean radius\\nwas 4.24 nm, and its SD was 0.49 nm.\\nNeural network performance\\nThe DEERNet result for Sample I is shown in Fig. 11. Apart from the\\nmore generous confidence intervals reported by the neural network en-\\nsemble, there isessentially no difference from the Tikhonov result—both\\nmajor distances are discovered and there is some uncertainty around the\\nbaseline. In this particular case, the performance of the two methods is\\nidentical up to the SD quoted.\\nIn Sample II, one label is situated in the structured part of the\\nN-terminal domain of LHCII (residue 34), whereas the other one is\\nsituated near the N terminus (residue 3) in a disordered region that ex-\\ntends at least to residue 12. A broad distance distribution, as it was found\\nbybothTikhonovregularization(Fig.10B)andtheneuralnetworks(Fig.12),\\nis expected. A bimodal distribution produced by DEERNet cannot be\\nexcluded a priori because the “correct” answer is not known in this case.\\nThe Tikhonov method performs better than neural networks for the\\nvery narrow and skewed distribution case seen in sample III (Fig. 13).\\nAlthough skewed distributionsare presentin the training database, neu-\\nral networks still predict a symmetric peak (at the right distance),\\nwhereas the Tikhonov output is correctly skewed, as expected for the\\nrigid linker between the two labels that behaves as a worm-like chain\\n(Fig. 10C). The likely reason for the loss of skew by the neural networks\\nis insufficient point count: Our networks are only 256 neurons wide, but\\nmore points are required to reproduce the sharp features seen in Fig.\\n10C. Networks that are 512 or 1024 neurons wide would likely get\\nthe skew right, but training these networks would require 10 times\\nthe processing power—this will have to wait until Tesla V100 cards ar-\\nrive at our local supercomputing center.\\n0\\n1\\n3\\n2\\n0\\n0.5\\n1\\n20\\n30\\n40\\n50\\n60\\n20\\n30\\n40\\n50\\n60\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nNetwork ensemble\\nEnsemble statistics\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\nExperimental data\\nFig. 14. DEERNet performance on Sample IV: [2]catenane (a pair of large interlocked rings) with a nitroxide spin label on each ring. The distance distribution is\\nin line with rough statistical estimates [Figure 5 in (65)], but there are fewer clumping artifacts compared to the output of the automatic Tikhonov regularization\\nprocedure. Within the Tikhonov framework, a manual regularization coefficient adjustment away from the corner of the L-curve is necessary to produce a distribution\\nfree of clumping artifacts.\\n0\\n0.5\\n1\\n1.5\\n0\\n0.5\\n1\\n20\\n30\\n40\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nNetwork ensemble\\nEnsemble statistics\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\nExperimental data\\n20\\n30\\n40\\nFig. 15. DEERNet performance on sample V: Pairs of nitroxide radicals tethered to the surface of gold nanoparticles, with the thiol tether attachment points\\ndiffusing on the surface of the nanoparticle (66). Note the markedly better performance relative to the Tikhonov method: The complete absence of clumping\\nartifacts and the remarkable match to the analytical model—down to the maximum exhibited by the broad feature around 35 Å.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n12 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 12}, page_content='Returningtobroaddistancedistributions,thetwointerlockedringsin\\n[2]catenane (Fig. 14) do perhaps push the limit of how broad a distance\\ndistribution between a pair of nitroxide radicals can be without any com-\\nplications associated with exchange couplings. The original paper (65)\\nreports statistical estimates of the distance distribution, but the one re-\\nported in that paper was based on the approximate Pake transformation\\nand therefore plagued by the subjective choice of distance-domain\\nsmoothing—a fairer comparison is to the present-day Tikhonov result\\nwith the regularization parameter determined by the L-curve, as shown\\nin Fig. 10D. Within the SDs quoted by both methods, the neural network\\noutput is not in any obvious way different from the Tikhonov regulariza-\\ntion result. For sample IV, both approaches perform equally well within\\nthe uncertainty expected for the true distribution.\\nHere, some discussion is in order about the choice of the regulariza-\\ntion parameter within the Tikhonov method. Although the L-curve\\ncriterion, on either the maximum curvature or the minimum distance\\nto the origin, looks reassuringly algebraic, its only real justification is\\nphilosophical—a balance must be struck between the quality of fit\\nand the regularization signal, and some humans have at some point\\ndecided that a few specific special points on the L-curve look like they\\nstrike a kind of balance. An element of human discretion is therefore\\nalways present in Tikhonov methods, as is evident from Fig. 1. Op-\\ntimal choice of the regularization parameter by different approaches\\nhas recently been studied for a large set of test data, and better options\\nthan L-curve–based criteria appear to exist (67). On the other hand, the\\nperformance of neural networks heavily depends on the quality and the\\nscope of the training set, which is also subject to human discretion. It\\nwould not, therefore, be fair to say that neural network results are entirely\\nfree of the human factor, but it is a human factor of a different kind.\\nThe most impressive performance of neural networks in our test set\\nis shown in Fig. 15—the relatively narrow peak sitting directly on top of\\na broad (but very real) pedestal. Tikhonov regularization has proven\\nFig. 16. Tikhonov distance distribution analysis for pairs of nitroxide radicals tethered to the surface of gold nanoparticles, with the thiol tether attachment\\npoints diffusing on the surface of the nanoparticle [sample Au3 after solvolysis and heating in (66)]. Green lines correspond to a model fit assuming a Gaussian\\ndistribution of distances and a homogeneous distribution of the biradicals on spherical nanoparticles with a Gaussian distribution of radii. Blue lines correspond to\\nTikhonov regularization with the regularization parameter in the L-curve corner as suggested by DeerAnalysis. Red lines correspond to Tikhonov regularization with a\\nlarger regularization parameter corresponding to the second L-curve corner. (A) Fits of the background-corrected DEER data (black). (B) Distance distributions. (C) L-curve and\\nthe two points selected for Tikhonov distance distribution analysis.\\n0\\n0.5\\n1\\nExperimental data\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nNetwork ensemble\\nEnsemble statistics\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\n0\\n2\\n4\\n30\\n40\\n50\\n60\\n70\\n30\\n40\\n50\\n60\\n70\\nFig. 17. DEERNet performance on sample VI: A rigid molecular triangle labeled with nitroxide radicals on two out of three corners (16).\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n13 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 13}, page_content='incapable of handling such cases [further examples may be found in\\n(68)], and neither of the two corners of the L-curve (or any point any-\\nwhere else, for that matter) produces the right answer, which we know\\nfrom fitting a parameterized model that agrees with known parameters\\nof the gold nanoparticles (Fig. 16B, green curve). When a broad peak\\noverlaps with a narrow one, the Tikhonov regularization parameter can\\nonly shift the solution between artificial broadening of the narrow peak\\nand artificial splitting in the broad peak. Neural networks confidently\\nproduce the right answer.\\nFinally, for sample VI, the results of Tikhonov regularization and\\nDEERNet agree rather nicely, except for a noise-related peak near 54 Å\\nand a minor peak near 30 Å that appear only in the Tikhonov-derived\\ndistribution. Width and shape of the main peak are rather similar. The\\nsignificance of the minor peak near 30 Å cannot be established, since\\nmolecular dynamics simulations performed for an isolated molecule at\\n298 K were not conclusive. Hence, the quality of the distance distribu-\\ntions generated by Tikhonov regularization and by the neural network\\nshould, in this case, be judged as similar.\\n0\\n0.5\\n1\\nInput data\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nRight answer\\nEnsemble statistics\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nAmplitude (a.u.)\\nMean\\n95%\\n0\\n0.5\\n1\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nAmplitude (a.u.)\\nMean\\n95%\\n0\\n0.5\\n1\\n1.5\\n2\\n0\\n0.5\\n1\\n20\\n30\\n40\\n50\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n20\\n30\\n40\\n50\\n0.1\\n0.2\\n0.3\\n0.4\\nDistance (Å)\\nDistance (Å)\\nTime (µs)\\nAmplitude (a.u.)\\nMean\\n95%\\nFig. 18. A demonstration of exchange coupling resilience. The networks were trained on the database where each DEER trace has an exchange coupling randomly\\nselected within the ±5-MHz interval (top row, J = –1.9 MHz; middle row, J = +2.9 MHz; bottom row, J = –3.6 MHz) and all other parameters as described in the “Training\\ndatabase generation” section. More than 99% of the training data set (including distributions with multiple distance peaks) produces the results of the kind shown in\\nthe top and middle panels—fast exchange oscillations are rejected and correct distance distributions are produced. With very noisy data (bottom), the networks duly\\nreport being highly uncertain.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n14 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 14}, page_content='On the basis of this small but very diverse set of test cases, we can\\nconclude that the performance of a neural network ensemble matches\\ntheperformance ofasoftwarepackage developedoveradecade. Tikhonov\\nregularization is better at reproducing the shape of very narrow dis-\\ntributions and possibly also for the broadest distribution encountered,\\nbut neural networks show much better performance for distributions\\nthat feature both narrow and broad components—a case that is likely\\nto occur in the context of order-disorder equilibria of proteins. Neural\\nnetworks also appear to have an advantage in rejection of small, noise-\\nrelated peaks. These features are particularly impressive when consid-\\nering that the networks can be trained in a matter of hours by an\\nunattended process. Given the close algebraic match described in Intro-\\nduction, this is perhaps to be expected. Still, this begs the question of\\nwhat wider and deeper networks with more sophisticated structure\\ncould accomplish. We do not, at the moment, have the computing\\npower to explore this matter, but the “noisy” appearance of some neural\\nnetwork outputs in Figs. 11 to 17 suggests that further improvements\\nare possible if the networks are trained longer and on larger data sets\\nthat are currently beyond the capacity of our Tesla cards.\\nExchange-resilient neural networks\\nNeural networks successfully process cases that are completely out of\\ndesign specifications of Tikhonov regularization methods—in this sec-\\ntion, we present the results of training an ensemble of networks on data\\nsets that include random interelectron exchange couplings selected\\nfrom the user-specified range (we have used ±5.0 MHz). Typical out-\\ncomes from previously unseen synthetic data sets are shown in the top\\nand middle rows of Fig. 18. Exchange-type distortions are prominent in\\nthe input DEER traces, but the answers produced by the networks are\\nnot perturbed.\\nTikhonov regularization with a dipolar kernel returns incorrect dis-\\ntance distributions (fig. S7), and this failure cannot be recognized by the\\nvalidation approach currently implemented in DeerAnalysis because\\nthe fit to the form factor can still appear to be good. Tikhonov methods\\nthat would account for the exchange coupling do not exist and would be\\nexceedingly hard to create because the exchange coupling effectively\\nadds the second dimension to the solution space.\\nIn contrast, only the correct distances are returned by the neural\\nnetworks. The rapid and slowly decaying modulation in the middle\\npanel should have produced a short distance with a sharp peak, yet\\nthe broad peak at a large distance is correctly identified. The networks\\nappear to learn the difference between sine/cosine and Fresnel mod-\\nulations in Eq. 2, and are able to demodulate the exchange component,\\nleaving only the dipolar part that is consistent between the sine/cosine\\nand the Fresnel parts.\\nThis is an impressive feat that makes DEER distance determination\\napplicabletoexchange-coupledsystemsthatarenotaccessibletoTikhonov\\nmethods. Even when the networks cannot make sense of the data due\\nto a combination of noise, exchange, and low modulation depth (Fig.\\n18, bottom), they still fail gracefully and report that none of the gen-\\nerated curve is certain. This being a clear extension of the available\\nDEER analysis functionality, exchange-resilient neural networks will\\nbe implemented into DeerAnalysis as an option in the near future.\\nIncluding exchange resilience into the training data set costs nothing\\nand introduces no extra work or adjustable parameters. The confidence\\nbounds on the distance distributions coming out of exchange-resilient\\nnetworks are wider, but that is to be expected because the uncertainty is\\nincreased. Another pertinent matter is that the exchange coupling can\\nitself be distance-dependent—our current training set assumes that it is\\nfixed. As long as the SD of the distribution is much smaller than its\\nmean, this is a reasonable assumption.\\nCONCLUSIONS AND OUTLOOK\\nThere is a straightforward map between the algebraic structure of the\\ntwo-electron dipolar spectroscopy analysis problem and the operations\\nperformed by artificial neural networks. When applied to the extraction\\nof distance distributions from DEER traces, this produces remarkably\\ngood performance that is on par with state-of-the-art tools. We strongly\\nrecommend neural networks for cases where narrow and broad features\\nare simultaneously present in the distance distribution. These cases can\\nbe identified by the inconclusive L-curve, such as the one in Fig. 16C.\\nNeural networks can also return a measure of uncertainty and learn\\npatterns of systematic distortions: A good example is the difference be-\\ntween an exchangecoupling(puresinusoidalpattern) andadipolarcou-\\npling (sinusoidal + Fresnel pattern). A sufficiently deep network trained\\non a representative data set is able to distinguish the two and return the\\ncorrect distance distribution even for exchange-coupled electrons.\\nAt a more abstract and speculative level, the procedure described in\\nthis work effectively converts the ability to simulate a physical process\\ninto the ability to interpret experimental data. In particular, a trained\\nneural network may be viewed as a Fredholm solver with a very general\\nkind of regularization. Where the Tikhonov method only incorporates\\none of the many physical insights that humans have about the solution\\n(namely, that it should be smooth and sparse), a perfectly trained neural\\nnetwork learns the entire class of admissible output patterns and only\\nlooks for solutions in that class. The challenge is rather to construct\\ntraining sets that completely cover both the solution space and the dis-\\ntortion space that one would encounter in practice.\\nSUPPLEMENTARY MATERIALS\\nSupplementary material for this article is available at http://advances.sciencemag.org/cgi/\\ncontent/full/4/8/eaat5218/DC1\\nSection S1. DEER kernel derivation\\nSection S2. Performance illustrations for networks of different depth\\nSection S3. Effects of transfer functions, choke points, and bias vectors\\nSection S4. Behavior of Tikhonov regularization for exchange-coupled systems\\nSection S5. Behavior of neural networks with the increasing level of noise\\nFig. S1. DEERNet performance illustration, distance distribution recovery: two-layer\\nfeedforward network, fully connected, with 256 neurons per layer.\\nFig. S2. DEERNet performance illustration, distance distribution recovery: three-layer\\nfeedforward network, fully connected, with 256 neurons per layer.\\nFig. S3. DEERNet performance illustration, distance distribution recovery: four-layer\\nfeedforward network, fully connected, with 256 neurons per layer.\\nFig. S4. DEERNet performance illustration, form factor recovery: two-layer feedforward\\nnetwork, fully connected, with 256 neurons per layer.\\nFig. S5. DEERNet performance illustration, form factor recovery: three-layer feedforward\\nnetwork, fully connected, with 256 neurons per layer.\\nFig. S6. DEERNet performance illustration, form factor recovery: four-layer feedforward\\nnetwork, fully connected, with 256 neurons per layer.\\nFig. S7. Tikhonov analysis of synthetic data produced as described in the main text and featuring a\\nunimodal distance distribution in the presence of a fixed exchange coupling (cf. Fig. 17).\\nFig. S8. A randomly generated DEER data set with the noise SD set at 2.5% of the modulation\\ndepth and the resulting distance distribution reconstruction by DEERNet.\\nFig. S9. A randomly generated DEER data set with the noise SD set at 10% of the modulation\\ndepth and the resulting distance distribution reconstruction by DEERNet.\\nFig. S10. A randomly generated DEER data set with the noise SD set at 30% of the modulation\\ndepth and the resulting distance distribution reconstruction by DEERNet.\\nTable S1. Distance distribution recovery performance statistics for feedforward networks with\\nhyperbolic tangent sigmoid (tansig) and logistic sigmoid (logsig) transfer function at the last layer.\\nTable S2. Performance statistics for a family of feedforward networks set up as a sequence of\\nfully connected layers with a choke point in the position indicated.\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n15 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 15}, page_content='REFERENCES AND NOTES\\n1. A. D. Milov, K. M. Salikhov, M. D. Shirov, Use of the double resonance in electron spin\\necho method for the study of paramagnetic center spatial distribution in solids.\\nFizika Tverdogo Tela 23, 975–982 (1981).\\n2. M. Pannier, S. Veit, A. Godt, G. Jeschke, H. W. Spiess, Dead-time free measurement of\\ndipole–dipole interactions between electron spins. J. Magn. Reson. 142, 331–340\\n(2000).\\n3. A. D. Milov, A. B. Ponomarev, Y. D. Tsvetkov, Electron-electron double resonance in\\nelectron spin echo: Model biradical systems and the sensitized photolysis of decalin.\\nChem. Phys. Lett. 110, 67–72 (1984).\\n4. G. Jeschke, A. Koch, U. Jonas, A. Godt, Direct conversion of EPR dipolar time evolution\\ndata to distance distributions. J. Magn. Reson. 155, 72–82 (2002).\\n5. S. Richert, J. Cremers, I. Kuprov, M. D. Peeks, H. L. Anderson, C. R. Timmel, Constructive\\nquantum interference in a bis-copper six-porphyrin nanoring. Nat. Commun. 8, 14842\\n(2017).\\n6. O. Schiemann, T. F. Prisner, Long-range distance determinations in biomacromolecules\\nby EPR spectroscopy. Q. Rev. Biophys. 40, 1–53 (2007).\\n7. G. Jeschke, DEER distance measurements on proteins. Annu. Rev. Phys. Chem. 63,\\n419–446 (2012).\\n8. S. Saxena, J. H. Freed, Double quantum two-dimensional Fourier transform electron spin\\nresonance: Distance measurements. Chem. Phys. Lett. 251, 102–110 (1996).\\n9. P. P. Borbat, J. H. Freed, Multiple-quantum ESR and distance measurements. Chem.\\nPhys. Lett. 313, 145–154 (1999).\\n10. L. V. Kulik, S. A. Dzuba, I. A. Grigoryev, Y. D. Tsvetkov, Electron dipole–dipole interaction in\\nESEEM of nitroxide biradicals. Chem. Phys. Lett. 343, 315–324 (2001).\\n11. S. Milikisyants, F. Scarpelli, M. G. Finiguerra, M. Ubbink, M. Huber, A pulsed EPR method to\\ndetermine distances between paramagnetic centers with strong spectral anisotropy and\\nradicals: The dead-time free RIDME sequence. J. Magn. Reson. 201, 48–56 (2009).\\n12. G. Jeschke, Dipolar spectroscopy—Double-resonance methods. eMagRes 5, 1459–1476 (2016).\\n13. G. Jeschke, G. Panek, A. Godt, A. Bender, H. Paulsen, Data analysis procedures for pulse\\nELDOR measurements of broad distance distributions. Appl. Magn. Reson. 26, 223 (2004).\\n14. Y.-W. Chiang, P. P. Borbat, J. H. Freed, The determination of pair distance distributions by\\npulsed ESR using Tikhonov regularization. J. Magn. Reson. 172, 279–295 (2005).\\n15. G. Jeschke, V. Chechik, P. Ionita, A. Godt, H. Zimmermann, J. Banham, C. Timmel, D. Hilger,\\nH. Jung, DeerAnalysis2006—A comprehensive software package for analyzing pulsed\\nELDOR data. Appl. Magn. Reson. 30, 473–498 (2006).\\n16. G. Jeschke, M. Sajid, M. Schulte, A. Godt, Three-spin correlations in double electron–\\nelectron resonance. Phys. Chem. Chem. Phys. 11, 6580–6591 (2009).\\n17. T. von Hagens, Y. Polyhach, M. Sajid, A. Godt, G. Jeschke, Suppression of ghost distances\\nin multiple-spin double electron–electron resonance. Phys. Chem. Chem. Phys. 15,\\n5854–5866 (2013).\\n18. A. Dalaloyan, M. Qi, S. Ruthstein, S. Vega, A. Godt, A. Feintuch, D. Goldfarb, Gd(III)-Gd(III)\\nEPR distance measurements—The range of accessible distances and the impact of zero\\nfield splitting. Phys. Chem. Chem. Phys. 17, 18464–18476 (2015).\\n19. N. Manukovsky, A. Feintuch, I. Kuprov, D. Goldfarb, Time domain simulation of Gd3+-Gd3+\\ndistance measurements by EPR. J. Chem. Phys. 147, 044201 (2017).\\n20. R. G. Larsen, D. J. Singel, Double electron–electron resonance spin–echo modulation:\\nSpectroscopic measurement of electron spin pair separations in orientationally\\ndisordered solids. J. Chem. Phys. 98, 5134–5146 (1993).\\n21. A. M. Bowen, C. E. Tait, C. R. Timmel, J. R. Harmer, Orientation-selective DEER using rigid\\nspin labels, cofactors, metals, and clusters, in Structural Information from Spin-Labels\\nand Intrinsic Paramagnetic Centres in the Biosciences, C. R. Timmel, J. R. Harmer, Eds.\\n(Springer, 2013), pp. 283–327.\\n22. G. Jeschke, Determination of the nanostructure of polymer materials by electron\\nparamagnetic resonance spectroscopy. Macromol. Rapid Commun. 23, 227–246\\n(2002).\\n23. B. E. Bode, J. Plackmeyer, M. Bolte, T. F. Prisner, O. Schiemann, PELDOR on an exchange\\ncoupled nitroxide copper (II) spin pair. J. Organomet. Chem. 694, 1172–1179 (2009).\\n24. D. F. Specht, A general regression neural network. IEEE Trans. Neural Netw. 2, 568–576\\n(1991).\\n25. H. J. Hogben, M. Krzystyniak, G. T. P. Charnock, P. J. Hore, I. Kuprov, Spinach—A software\\nlibrary for simulation of spin dynamics in large spin systems. J. Magn. Reson. 208,\\n179–194 (2011).\\n26. K. Salikhov, S.-A. Dzuba, A. M. Raitsimring, The theory of electron spin-echo signal decay\\nresulting from dipole-dipole interactions between paramagnetic centers in solids.\\nJ. Magn. Reson. 42, 255–276 (1981).\\n27. A. D. Milov, Y. D. Tsvetkov, Double electron-electron resonance in electron spin echo:\\nConformations of spin-labeled poly-4-vinilpyridine in glassy solutions. Appl. Magn.\\nReson. 12, 495–504 (1997).\\n28. D. R. Kattnig, J. Reichenwallner, D. Hinderberger, Modeling excluded volume effects for\\nthe faithful description of the background signal in double electron–electron resonance.\\nJ. Phys. Chem. B 117, 16542–16557 (2013).\\n29. G. Jeschke, Y. Polyhach, Distance measurements on spin-labelled biomacromolecules by\\npulsed electron paramagnetic resonance. Phys. Chem. Chem. Phys. 9, 1895–1910 (2007).\\n30. M. K. Bowman, A. G. Maryasov, N. Kim, V. J. DeRose, Visualization of distance distribution\\nfrom pulsed double electron-electron resonance data. Appl. Magn. Reson. 26, 23 (2004).\\n31. P. C. Hansen, Analysis of discrete ill-posed problems by means of the L-curve.\\nSIAM Rev. 34, 561–580 (1992).\\n32. S. A. Dzuba, The determination of pair-distance distribution by double electron–electron\\nresonance: Regularization by the length of distance discretization with Monte Carlo\\ncalculations. J. Magn. Reson. 269, 113–119 (2016).\\n33. M. Srivastava, J. H. Freed, Singular value decomposition method to determine distance\\ndistributions in pulsed dipolar electron spin resonance. J. Phys. Chem. Lett. 8, 5648–5655\\n(2017).\\n34. E. A. Suturina, D. Haussinger, K. Zimmermann, L. Garbuio, M. Yulikov, G. Jeschke, I. Kuprov,\\nModel-free extraction of spin label position distributions from pseudocontact shift data.\\nChem. Sci. 8, 2751–2757 (2017).\\n35. H. Schäfer, B. Mädler, E. Sternin, Determination of orientational order parameters from 2H\\nNMR spectra of magnetically partially oriented lipid bilayers. Biophys. J. 74, 1007–1014\\n(1998).\\n36. T. H. Edwards, S. Stoll, A Bayesian approach to quantifying uncertainty from experimental\\nnoise in DEER spectroscopy. J. Magn. Reson. 270, 87–97 (2016).\\n37. G. Jeschke, M. Sajid, M. Schulte, N. Ramezanian, A. Volkov, H. Zimmermann, A. Godt,\\nFlexibility of shape-persistent molecular building blocks composed of p-phenylene and\\nethynylene units. J. Am. Chem. Soc. 132, 10107–10117 (2010).\\n38. G. Jeschke, Interpretation of dipolar EPR data in terms of protein structure,\\nin Structural Information from Spin-Labels and Intrinsic Paramagnetic\\nCentres in the Biosciences, C. R. Timmel, J. R. Harmer, Eds. (Springer, 2011),\\npp. 83–120.\\n39. K. Ackermann, C. Pliotas, S. Valera, J. H. Naismith, B. E. Bode, Sparse labeling PELDOR\\nspectroscopy on multimeric mechanosensitive membrane channels. Biophys. J. 113,\\n1968–1978 (2017).\\n40. M. R. Cohen, V. Frydman, P. Milko, M. A. Iron, E. H. Abdelkader, M. D. Lee, J. D. Swarbrick,\\nA. Raitsimring, G. Otting, B. Graham, A. Feintuch, D. Goldfarb, Overcoming artificial\\nbroadening in Gd3+–Gd3+ distance distributions arising from dipolar pseudo-secular\\nterms in DEER experiments. Phys. Chem. Chem. Phys. 18, 12847–12859 (2016).\\n41. A. Collauto, V. Frydman, M. Lee, E. Abdelkader, A. Feintuch, J. D. Swarbrick, B. Graham,\\nG. Otting, D. Goldfarb, RIDME distance measurements using Gd(iii) tags with a narrow\\ncentral transition. Phys. Chem. Chem. Phys. 18, 19037–19049 (2016).\\n42. S. Razzaghi, M. Qi, A. I. Nalepa, A. Godt, G. Jeschke, A. Savitsky, M. Yulikov, RIDME\\nspectroscopy with Gd (III) centers. J. Phys. Chem. Lett. 5, 3970–3975 (2014).\\n43. K. Keller, V. Mertens, M. Qi, A. I. Nalepa, A. Godt, A. Savitsky, G. Jeschke, M. Yulikov,\\nComputing distance distributions from dipolar evolution data with overtones: RIDME\\nspectroscopy with Gd(iii)-based spin labels. Phys. Chem. Chem. Phys. 19, 17856–17876\\n(2017).\\n44. A. Godt, M. Schulte, H. Zimmermann, G. Jeschke, How flexible are poly(para-\\nphenyleneethynylene)s? Angew. Chem. Int. Ed. 118, 7722–7726 (2006).\\n45. A. G. Matveeva, V. M. Nekrasov, A. G. Maryasov, Analytical solution of the PELDOR inverse\\nproblem using the integral Mellin transform. Phys. Chem. Chem. Phys. 19, 32381–32388\\n(2017).\\n46. K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal\\napproximators. Neural Netw. 2, 359–366 (1989).\\n47. W. S. McCulloch, W. Pitts, A logical calculus of the ideas immanent in nervous activity.\\nBull. Math. Biophys. 5, 115–133 (1943).\\n48. M. F. Møller, A scaled conjugate gradient algorithm for fast supervised learning.\\nNeural Netw. 6, 525–533 (1993).\\n49. M. Riedmiller, H. Braun, A direct adaptive method for faster backpropagation learning:\\nThe RPROP algorithm, in IEEE International Conference on Neural Networks (IEEE, 1993),\\npp. 586–591.\\n50. K.-I. Funahashi, On the approximate realization of continuous mappings by neural\\nnetworks. Neural Netw. 2, 183–192 (1989).\\n51. V. Kurková, Surrogate solutions of Fredholm equations by feedforward networks.\\nITAT Conf. Proc. 49–54 (2012).\\n52. G. Gnecco, V. Kůrková, M. Sanguineti, Accuracy of approximations of solutions to\\nFredholm equations by kernel methods. Appl. Math Comput. 218, 7481–7497\\n(2012).\\n53. A. R. Barron, Universal approximation bounds for superpositions of a sigmoidal function.\\nIEEE Trans. Inf. Theor. 39, 930–945 (1993).\\n54. A. Jafarian, S. M. Nia, Utilizing feed-back neural network approach for solving linear\\nFredholm integral equations system. App. Math. Model. 37, 5027–5038 (2013).\\n55. S. Effati, R. Buzhabadi, A neural network approach for solving Fredholm integral\\nequations of the second kind. Neural Comput. Appl. 21, 843–852 (2012).\\n56. B. Asady, F. Hakimzadegan, R. Nazarlue, Utilizing artificial neural network approach for\\nsolving two-dimensional integral equations. Math. Sci. 8, 117 (2014).\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n16 of 17\\nDownloaded from https://www.science.org on July 09, 2025'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 10.1.16 (Windows); modified using iText 4.2.0 by 1T3XT', 'creator': 'Arbortext Advanced Print Publisher 9.1.440/W Unicode', 'creationdate': '2018-08-18T01:24:14+08:00', 'source': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'file_path': '..\\\\data\\\\pdf\\\\sciadv.aat5218.pdf', 'total_pages': 17, 'format': 'PDF 1.3', 'title': 'Deep neural network processing of DEER data', 'author': '', 'subject': 'Sci. Adv. 2018.4:eaat5218', 'keywords': '', 'moddate': '2025-07-09T06:13:14-07:00', 'trapped': '', 'modDate': \"D:20250709061314-07'00'\", 'creationDate': \"D:20180818012414+08'00'\", 'page': 16}, page_content='57. Y. Ma, J. Huang, H. Li, A novel numerical method of two-dimensional Fredholm integral\\nequations of the second kind. Math. Probl. Eng. 2015, 625013 (2015).\\n58. S. Pribitzer, M. Sajid, M. Hülsmann, A. Godt, G. Jeschke, Pulsed triple electron resonance (TRIER)\\nfor dipolar correlation spectroscopy. J. Magn. Reson. 282, 119–128 (2017).\\n59. T. Hastie, R. Tibshirani, J. Friedman, Overview of supervised learning, in The Elements of\\nStatistical Learning, T. Hastie, R. Tibshirani, J. Friedman, Eds. (Springer, 2009), pp. 9–41.\\n60. I. Kuprov, Fokker-Planck formalism in magnetic resonance simulations. J. Magn. Reson.\\n270, 124–135 (2016).\\n61. A. O’Hagan, T. Leonard, Bayes estimation subject to uncertainty about parameter\\nconstraints. Biometrika 63, 201–203 (1976).\\n62. D. J. Heyes, B. Khara, M. Sakuma, S. J. O. Hardman, R. O’Cualain, S. E. J. Rigby, N. S. Scrutton,\\nUltrafast red light activation of synechocystis phytochrome Cph1 triggers major structural\\nchange to form the Pfr signalling-competent state. PLOS ONE 7, e52418 (2012).\\n63. B. E. Rosen, Ensemble learning using decorrelated neural networks. Conn. Sci. 8, 373–384\\n(1996).\\n64. N. Fehr, C. Dietz, Y. Polyhach, T. von Hagens, G. Jeschke, H. Paulsen, Modeling of the\\nN-terminal section and the lumenal loop of trimeric light harvesting complex II (LHCII) by\\nusing EPR. J. Biol. Chem. 290, 26007–26020 (2015).\\n65. G. Jeschke, A. Godt, Co-conformational distribution of nanosized [2]catenanes\\ndetermined by pulse EPR measurements. ChemPhysChem 4, 1328–1334 (2003).\\n66. P. Ionita, A. Volkov, G. Jeschke, V. Chechik, Lateral diffusion of thiol ligands on the surface of Au\\nnanoparticles: An electron paramagnetic resonance study. Anal. Chem. 80, 95–106 (2008).\\n67. T. H. Edwards, S. Stoll, Optimal Tikhonov regularization for DEER spectroscopy.\\nJ. Magn. Reson. 288, 58–68 (2018).\\n68. A. L. Lai, E. M. Clerico, M. E. Blackburn, N. A. Patel, C. V. Robinson, P. P. Borbat, J. H. Freed,\\nL. M. Gierasch, Key features of an Hsp70 chaperone allosteric landscape revealed by\\nion-mobility native mass spectrometry and double electron-electron resonance.\\nJ. Biol. Chem. 292, 8773–8785 (2017).\\nAcknowledgments: We thank T. von Hagens for providing the DEER data for samples I and II\\n(64) and A. Volkov for providing the data for sample V (66). The authors are grateful to\\nR. Bittl for stimulating discussions. We also acknowledge the use of the IRIDIS High\\nPerformance Computing Facility and associated services at the University of Southampton.\\nFunding: This work was supported by the Engineering and Physical Sciences Research\\nCouncil (EP/N006895/1). Author contributions: The contributions from all four authors are\\nsignificant, closely intertwined, and impossible to disentangle. Competing interests: The\\nauthors declare that they have no competing interests. Data and materials availability: All\\ndata needed to evaluate the conclusions in the paper are present in the paper and/or the\\nSupplementary Materials. Additional data and software related to this paper may be requested\\nfrom the authors.\\nSubmitted 7 March 2018\\nAccepted 20 July 2018\\nPublished 24 August 2018\\n10.1126/sciadv.aat5218\\nCitation: S. G. Worswick, J. A. Spencer, G. Jeschke, I. Kuprov, Deep neural network processing\\nof DEER data. Sci. Adv. 4, eaat5218 (2018).\\nS C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E\\nWorswick et al., Sci. Adv. 2018;4:eaat5218\\n24 August 2018\\n17 of 17\\nDownloaded from https://www.science.org on July 09, 2025')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
